{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13169ca-164d-406c-a3ba-159dfc5d0943",
   "metadata": {},
   "source": [
    "# Makemore - Initialization and BatchNorm\n",
    "\n",
    "This notebook is a summary and re-implementation of Andrej Karpathy's [Building makemore Part 3: Activations & Gradients, BatchNorm](https://youtu.be/P6sfmUTpUmc?si=epSJSH4sBF08J_SO) as part of the Neural Networks: Zero to Hero series.\n",
    "\n",
    "There are 5 main takeaways from this lecture:\n",
    "1. Why bad initialization can harm neural network training\n",
    "2. How to find correct initialization values (Kaiming He Init)\n",
    "3. Introduction to BatchNorm\n",
    "4. Diagnostic tools to determine health of NN\n",
    "5. Converting our code into a PyTorch-like API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d81b7-42c5-4ab3-99d4-16fa1b54a590",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df123456-bb4a-40e6-b025-994c58c91299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015660ea-4354-4bcf-9c97-fe9d91c7e41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b315b48-7ba7-4b9e-8ad9-b3997b31c2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ad6782-d2b8-4d10-92fa-8a56847e5f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c694e47-fdf2-4375-bfb7-2d0cdd018da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "block_size = 3\n",
    "n_embed = 10\n",
    "n_hidden = 100\n",
    "max_steps = 200000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519ca018-4016-48a4-8c87-6d36e4d1dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile dataset\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "# 80, 10, 10 split\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "train_X, train_Y = build_dataset(words[:n1])\n",
    "valid_X, valid_Y = build_dataset(words[n1:n2])\n",
    "test_X, test_Y = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee470f-5755-48ce-b806-5275314b180e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problems with the previous training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe930859-08d7-4b6a-807a-a60103cf4730",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Softmax being confidently wrong\n",
    "\n",
    "If the output layer just before softmax provides large values, softmax will be confidently wrong, leading to a high inital loss. Let's see this by breaking out of the training loop after a single iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d263d64-3f6e-463f-bde6-66a0303e21d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5997\n"
     ]
    }
   ],
   "source": [
    "# set up parameters\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, n_embed), generator=g)\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden, generator=g)\n",
    "W2 = torch.randn((n_hidden, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "\n",
    "parameters = [C, W1, W2, b2]\n",
    "\n",
    "print(sum(p.nelement() for p in parameters)) # total number of parameters\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9acbea7e-24f4-455a-858a-d1276a354a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 18.8487\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    # construct minibatch\n",
    "    ix = torch.randint(0, train_X.shape[0], (batch_size,))\n",
    "    Xb, Yb = train_X[ix], train_Y[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    hpreact = emb.view(-1, n_embed * block_size) @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5e18b-f12c-4d99-b97a-3ea86fd1e70b",
   "metadata": {},
   "source": [
    "We see that the initial loss is ~17. Remember that the loss is the negative log of the probability of the correct token. Because the network is not yet trained, there is no reason for the network to believe that some character should be any more likely than any other character. So the expected loss should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a65716-43d0-4ae1-9e3b-6e7c56bd93a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2958)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.tensor(1/27.0).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435434e-dacf-47e5-afc5-9e3ec1d1f057",
   "metadata": {},
   "source": [
    "So we see that our initial loss is much larger than the expected loss. This occurs because the magnitude of our initial weights is large, so softmax gives confidently incorrect predictions. The problem with this is that training cycles are wasted on simply bringing down the magnitude of the weights, and if we plot the loss over time, we would get a hockey stick-like graph due to easy gains in the beginning, which squash the (more relevant) later parts of training. We can easily fix this by reducing the weights and biases in the output layer by multiplying the weights by a small constant (e.g. 0.01) and setting the biases to 0.\n",
    "\n",
    "We can re-initialize with smaller output layer parameters and verify the initial loss being roughly equal to the expected loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b51e10a6-4e6a-488f-a353-b9e8a935ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5997\n"
     ]
    }
   ],
   "source": [
    "# set up parameters\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, n_embed), generator=g)\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden, generator=g)\n",
    "W2 = torch.randn((n_hidden, 27), generator=g) * 0.01\n",
    "b2 = torch.randn(27, generator=g) * 0\n",
    "\n",
    "parameters = [C, W1, W2, b2]\n",
    "\n",
    "print(sum(p.nelement() for p in parameters)) # total number of parameters\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2848ebe1-8839-46d7-b7a7-776ed148b04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2993\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    # construct minibatch\n",
    "    ix = torch.randint(0, train_X.shape[0], (batch_size,))\n",
    "    Xb, Yb = train_X[ix], train_Y[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    hpreact = emb.view(-1, n_embed * block_size) @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75a39b-47fe-44ba-bc98-267bde487843",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Saturated Gradients\n",
    "\n",
    "The saturated gradient problem is when the pre-activations that feed into an activation layer such as `tanh` or `sigmoid` take on extreme values (very positive or very negative) and have very small gradients, which causes very small updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8169dc-b4d5-4bfd-9a62-bf11b6f49e24",
   "metadata": {},
   "source": [
    "For example, if the activation value `t` is close to -1 or 1, you are on the flat region of the `tanh` graph, where the gradient is almost zero. This would cut off the gradient flowing backward here and cause the neuron to not update (if this happened to the neuron for all examples, the neuron would never update, which is known as a \"dead neuron\"). Another way to see this is through the backprop update rule for `tanh`: `self.grad += (1-t**2) * out.grad` where `self.grad` would be 0 regardless of the gradient of its parents.\n",
    "\n",
    "Let's see this happen in our previous implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "783b7086-eb6a-4bb1-946f-70f2960505ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5997\n"
     ]
    }
   ],
   "source": [
    "# set up parameters\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, n_embed), generator=g)\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden, generator=g)\n",
    "W2 = torch.randn((n_hidden, 27), generator=g) * 0.01\n",
    "b2 = torch.randn(27, generator=g) * 0\n",
    "\n",
    "parameters = [C, W1, W2, b2]\n",
    "\n",
    "print(sum(p.nelement() for p in parameters)) # total number of parameters\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53f2aa5-c46d-4120-861e-9c9c1165cec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3118\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    # construct minibatch\n",
    "    ix = torch.randint(0, train_X.shape[0], (batch_size,))\n",
    "    Xb, Yb = train_X[ix], train_Y[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    hpreact = emb.view(-1, n_embed * block_size) @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b048297-2b39-45ed-aa61-db5175cafda8",
   "metadata": {},
   "source": [
    "Let's plot the activations values after the first layer (`h`) and see how many of them took on extreme values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d1c861-fb4f-425f-bc15-e37e060e6f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(h.abs() > 0.99, cmap='gray', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b7613-0638-4381-bff4-f9d1bc482d02",
   "metadata": {},
   "source": [
    "The white cells represent examples for which the activation was saturated. We can see that more than half of the cells are saturated, which would hinder training. (We are working with a very small network which is easier to train / more tolerant to bad initialization, but for larger networks, saturated gradients will likely halt training completely)\n",
    "\n",
    "So how do we solve this problem? We want to lower the magnitude of the pre-activation values. Similar to above, we can multiply the weights and biases by some small value to achieve this. The `Kaiming He initialization` method is a popular method that provides a systematic way to do this that provides good activation values throughout the network. We multiply the weights with $\\frac{gain}{\\sqrt{input\\_dim}}$ where $gain$ is a constant depending on the type of activation function.\n",
    "\n",
    "The intuition behind the gain multiplier is to fight the contractive nature of activation functions. Assume you have multiple blocks of a linear layer followed by an activation function. Imagine the first pass through the network in which all weights were initialized without a gain. The output coming out of each block will be slightly squashed and will converge to 0 as we go deeper into the network. To fight the effect of the contraction, we need to boost the weights by a gain to renormalize everything back to unit standard deviation.\n",
    "\n",
    "We are intentionally skipping over a lot of details here, because the paper is more mathematically involved, and modern innovations such as BatchNorm, RMSProp, and Adam made neural network training more robust to bad initialization.\n",
    "\n",
    "Another solutinon is to use a different activation function such as `Leaky ReLU`, which does not have a flat region and is less susceptible to dying gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e16efa2a-383c-4be8-9bbc-d0aee9d5b2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5997\n"
     ]
    }
   ],
   "source": [
    "# set up parameters\n",
    "TANH_GAIN = 5/3\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, n_embed), generator=g)\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden), generator=g) * TANH_GAIN/((n_embed*block_size)**0.5) # Kaiming He Init\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "W2 = torch.randn((n_hidden, 27), generator=g) * 0.01\n",
    "b2 = torch.randn(27, generator=g) * 0\n",
    "\n",
    "parameters = [C, W1, W2, b2]\n",
    "\n",
    "print(sum(p.nelement() for p in parameters)) # total number of parameters\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d23bc449-757f-4dad-a217-b627ebd8569a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3027\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    # construct minibatch\n",
    "    ix = torch.randint(0, train_X.shape[0], (batch_size,))\n",
    "    Xb, Yb = train_X[ix], train_Y[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    hpreact = emb.view(-1, n_embed * block_size) @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf5150ac-326b-417d-a9a5-d83bb4b4e5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(h.abs() > 0.99, cmap='gray', interpolation='nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498b449-18aa-4d51-bf22-1638fec8dd48",
   "metadata": {},
   "source": [
    "We see that we get a lot less activated neurons with the Kaiming He initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8996b3b-0068-44b4-b7eb-b46d75c52d96",
   "metadata": {},
   "source": [
    "## BatchNorm\n",
    "BatchNorm is a technique due to a 2015 paper by Ioffe and Szegedy from Google Research that made it possible to train very deep models reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463fc672-4d62-464c-88a7-2197b9e22c6f",
   "metadata": {},
   "source": [
    "The idea is that having roughty unit Gaussian pre-activations is good because many of them land in the non-flat region of the activation function. So we just take those pre-activations and normalize them (along each dimension of the batch) before passing them through the activation function. However, if we simply normalize them to be unit Gaussian, then most examples will fall in the range `[-1, 1]` where the activation function is mostly linear. This makes us lose the non-linearity in the network which is what allows the neural net to learn arbitrary functions. Therefore, we re-scale and re-shift the normalized values before passing them into the activation function. Because we do not know how much to re-scale and re-shift, we set them as trained parameters.\n",
    "\n",
    "The pseudocode is as follows:\n",
    "\n",
    "`for` each dimension of the input `do`\n",
    "1. Compute mean over batch\n",
    "2. Compute std over batch\n",
    "3. Normalize each value by subtracting mean and dividing by std\n",
    "4. Scale and shift by learned parameters $\\gamma$ and $\\beta$\n",
    "\n",
    "Batch normalization allows us to control statistics of activations in the neural net. BN layers are commonly placed after linear layers or a convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000c78ac-4526-427e-9285-9dc93027b682",
   "metadata": {},
   "source": [
    "Note that a BN layer requires the batch mean and batch variance to first normalize the inputs to unit Gaussian. For a training batch, we can simply compute the values over, but during test time, we may not be working with batches, and we need the values over the entire dataset. We do not like to have an extra step to compute the mean and variance over the dataset at the end, so we estimate them using an exponential moving average during training (this is what PyTorch does). Note that the EMA estimation is not part of backpropagation but just another computation on the side.\n",
    "\n",
    "Also, when we are using a BN layer, the bias in the preceding layer is canceled out, so we can remove it to reduce unnecessary computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4538ac-581f-4775-b819-e55c0c8dc85b",
   "metadata": {},
   "source": [
    "One strange thing that is happening with batch normalization is that examples in a batch are now coupled in the overall activation. Previously, we were simply batching examples for efficiency but each example was processed independently of others. This leads to many bugs, but BN works so well empirically that it is keep being used. One reason for the good performance is that BN works as a regularizer that adds noise, and makes it harder for the neural network to overfit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c973835-7d7e-4093-9854-ddd10fa04db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6197\n"
     ]
    }
   ],
   "source": [
    "# set up parameters\n",
    "TANH_GAIN = 5/3\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, n_embed), generator=g)\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden), generator=g) * TANH_GAIN/((n_embed*block_size)**0.5) # Kaiming He Init\n",
    "W2 = torch.randn((n_hidden, 27), generator=g) * 0.01\n",
    "b2 = torch.randn(27, generator=g) * 0\n",
    "\n",
    "# BN params\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "\n",
    "print(sum(p.nelement() for p in parameters)) # total number of parameters\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d157073-df39-4011-ae97-963500fc43fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.2928\n",
      "  10000/ 200000: 1.9752\n",
      "  20000/ 200000: 2.1940\n",
      "  30000/ 200000: 2.1310\n",
      "  40000/ 200000: 2.4957\n",
      "  50000/ 200000: 1.8258\n",
      "  60000/ 200000: 2.0837\n",
      "  70000/ 200000: 1.9849\n",
      "  80000/ 200000: 2.5758\n",
      "  90000/ 200000: 1.7949\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    # construct minibatch\n",
    "    ix = torch.randint(0, train_X.shape[0], (batch_size,))\n",
    "    Xb, Yb = train_X[ix], train_Y[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    hpreact = emb.view(-1, n_embed * block_size) @ W1# + b1\n",
    "    # Batchnorm Layer\n",
    "    # -------------------------------------------------------------------\n",
    "    bnmeani = hpreact.mean(0, keepdim=True)\n",
    "    bnstdi = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias # we are skipping addition of epsilon in the denominator\n",
    "    \n",
    "    # approximate mean/std of dataset during train time\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "        bnstd_running = 0.999 * bnmean_running + 0.001 * bnstdi\n",
    "    # -------------------------------------------------------------------\n",
    "    \n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4c085-f13d-420b-888e-2ee902807c76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to PyTorch API\n",
    "The PyTorch API provided by `torch.nn` gives access to classes that can be used as layers in a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f17ee6-7e32-49d7-8352-4065572ca31a",
   "metadata": {},
   "source": [
    "PyTorch's `nn.Linear` module takes in `fan_in` and `fan_out` (number of input and output dimensions) and initializes weights by sampling from a uniform distribution over `(-sqrt(k), sqrt(k))` where $k=\\frac{1}{fan\\_in}$ (the scaling by `sqrt(k)` if roughly along the lines of the Kaiming He initialization seen above).\n",
    "\n",
    "PyTorch's `nn.BatchNorm1d` takes keyword arguments `eps` (typically not changed), `momentum` (used for computing running mean and std -- use smaller momentum if using small batch sizes), `affine` determines whether or not to add learnable gain and bias to the batch norm layer (default is `True` and should almost always be `True`), `track_running_stats` detemines whether to compute the running mean and std.\n",
    "\n",
    "We try to replicate the PyTorch modules in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad443bf-dc93-430c-9c8f-491a9153358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in ** 0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "\n",
    "class BatchNorm1d:\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True # NOTE: control behavior depending on training or evaluation\n",
    "        # parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # buffers (trained with a running 'momentum update')\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True)\n",
    "        else:\n",
    "            xmean = x.running_mean\n",
    "            xvar = x.running_var\n",
    "        \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        # update buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum + xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum + xvar\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "\n",
    "class Tanh:\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce45da72-248b-4ccb-9c76-c9160042b57f",
   "metadata": {},
   "source": [
    "## Diagnostic tools to understand the health of a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be73ef1-447f-4eac-8288-4853ccc3be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embed), generator=g)\n",
    "layers = [\n",
    "    Linear(n_embed * block_size, n_hidden), Tanh(),\n",
    "    Linear(          n_hidden, n_hidden), Tanh(),\n",
    "    Linear(          n_hidden, n_hidden), Tanh(),\n",
    "    Linear(          n_hidden, n_hidden), Tanh(),\n",
    "    Linear(          n_hidden, n_hidden), Tanh(),\n",
    "    Linear(          n_hidden, vocab_size),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # make last softmax layer less confident\n",
    "    layers[-1].weight *= 0.1\n",
    "    # for other layers, apply gain\n",
    "    for layer in layers[:-1]:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= 5/3\n",
    "            \n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ccbd5-cfda-40de-9884-e62c19d7129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    \n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, train_X.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = train_X[ix], train_Y[ix]\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad() # AFTER_DEBUG: would take out retain_grad\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # LR decay\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        # NOTE: I am not sure why we are using std here to meausre the grad:data ratio, I think it should be norm/mean\n",
    "        ud.append([(lr * p.grad.std() / p.data.std()).log10().item() for p in parameters])\n",
    "    \n",
    "    if i > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64a725-fa64-4e8c-926d-695a5e9e4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize activations from Tanh layers\n",
    "plt.figure(figsize=(20,4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out\n",
    "        print(\"layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%\" % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean() * 100))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__})')\n",
    "\n",
    "plt.legend(legends)\n",
    "plt.title(\"activation distribution (forward pass)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0d48ec-f84f-4f47-8f8a-6cb6a0e6f451",
   "metadata": {},
   "source": [
    "We see that the first layer is quite saturated, but stabilizes in the later layers, which looks healthy. We would not want the saturation to be too high or too low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef636d9-2984-41fc-bb8d-55681f266083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize gradients from Tanh layers\n",
    "plt.figure(figsize=(20,4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out.grad\n",
    "        print(\"layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%\" % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean() * 100))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__})')\n",
    "\n",
    "plt.legend(legends)\n",
    "plt.title(\"gradient distribution (backward pass)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78611776-d498-42c1-8ab0-248485e51bcc",
   "metadata": {},
   "source": [
    "Main thing to note from this plot is that the gradients in different layers have similar values (desirable). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8eca82-d96f-4f89-aecd-ffabf07d482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize gradients of parameters\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, p in enumerate(parameters):\n",
    "    t = p.grad\n",
    "    if p.ndim == 2: # only focus on 2-dimensional params -- i.e. weights, not biases\n",
    "        # NOTE: I am not sure why we are using std here to meausre the grad:data ratio, I think it should be norm/mean\n",
    "        print(\"weight %10s | mean %+f | std %e | grad:data ratio %e\" % (tuple(p.shape), t.mean(), t.std(), t.mean() / p.mean()))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends)\n",
    "plt.title('weights gradient distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78763d7a-e6d1-496c-9ca7-eff44e242255",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "legends = []\n",
    "for i, p in enumerate(parameters):\n",
    "    if p.ndim == 2:\n",
    "        plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "        legends.append(\"param %d\" % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3 (heuristically), indicate on plot\n",
    "plt.legend(legends);\n",
    "# NOTE: this is also a good way to determine learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e86177-f984-4441-be12-c2b924364a3c",
   "metadata": {},
   "source": [
    "Using BN layers make the network much more robust to bad initialization. All the plots above, except the last, will still look similar if we removed the weight adjustments (multiplying gain). The update to data ratios will still be bad if we do not have good initialization values, but that can be fixed by simply tuning the learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
