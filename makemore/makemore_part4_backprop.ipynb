{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6f5ace-469a-43e9-b639-808c7919e7b4",
   "metadata": {},
   "source": [
    "# Makemore Part 4 - Becoming a Backprop Ninja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52eb43-5861-4608-a026-b81be1e588d5",
   "metadata": {},
   "source": [
    "This notebook is a summary of Andrej Karpathy's [Building makemore Part 4: Becoming a Backprop Ninja](https://youtu.be/q8SA3rM6ckI?si=wF75sMbXpgDd6U1y) as part of the Neural Networks: Zero to Hero series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c244f31-4236-4e4f-8ad7-1132bd088fde",
   "metadata": {},
   "source": [
    "All the major machine learning libraries/frameworks have an autograd engine that automatically computes gradients using backpropagation. However, backpropagation is a leaky abstraction - we need to understand what is going on under the hood to use it effectively and debug problems.\n",
    "\n",
    "Previously, we implemented `micrograd` which was a scalar-level autograd engine, but we would now like to extend that to the level of tensors. In this notebook, we try to backpropagate by hand our network from part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eaae3d-4979-45ea-a34d-e6083e209818",
   "metadata": {},
   "source": [
    "Fun fact: In the past, everyone wrote backward passes for their network by hand, instead of using autograd engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7bfab28-eb90-45ca-bb83-635549689e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- SETUP CODE ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b56d84bf-51c8-43b3-acfc-e5df36b5a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f891baf1-5b2b-4dc7-aab3-c5c0d2983eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# set up dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29e81c2-40f8-43ef-b14f-0c9d14bd1958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a557168-2470-41ea-a6b8-8eabf59931bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "  \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e5c90b-5b2b-4e37-b27f-462a94f99394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    # dt is our calculated gradient, t is pytorch's gradient\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | shape_match: {str(dt.shape == t.shape):6s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d53aed7-a633-459b-ade2-a52c515d71f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a84519d-59ee-4437-a456-b07fd69d7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e14e536f-cb74-4980-9699-2d31b3394a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- SETUP CODE END ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5513194-5127-4914-8b65-dcf69cfc387b",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b77c3fd-940f-4545-864f-2d61bc46e19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3353, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8fbd5a5-f93c-4c79-bed1-c66e2643f81c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | shape_match: True   | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# -----------------\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n),Yb] = -1.0/n\n",
    "\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "\n",
    "dnorm_logits = norm_logits.exp() * dcounts\n",
    "\n",
    "dlogit_maxes = -dnorm_logits.sum(1, keepdim=True)\n",
    "\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "# be careful of dimension of broadcasting\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "dhpreact = dh * (1 - h**2)\n",
    "\n",
    "dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
    "dbnraw = dhpreact * bngain\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "\n",
    "dbndiff = dbnraw * bnvar_inv\n",
    "dbnvar_inv = (dbnraw * bndiff).sum(0, keepdim=True)\n",
    "\n",
    "dbnvar = -0.5 * (bnvar + 1e-5) ** -1.5 * dbnvar_inv\n",
    "\n",
    "dbndiff2 = 1/(n-1) * torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += 2 * dbndiff2 * bndiff\n",
    "\n",
    "dbnmeani = -dbndiff.sum(0, keepdim=True)\n",
    "dhprebn = dbndiff.clone()\n",
    "\n",
    "dhprebn += 1/n * torch.ones_like(hprebn) * dbnmeani\n",
    "\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "\n",
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "# emb = C[Xb]\n",
    "# Xb is (32,3), each element being a number that is used to index into C.\n",
    "# emb is (32,3,10), where for each element in Xb, we have the embedding\n",
    "# For indices i,j, demb[i,j] represents the gradients for each element of the embedding of the character X[i,j]\n",
    "# Need to accumulate gradients because rows will be reused\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3735db18-eb6e-4e8f-b9d4-5da7d11157d6",
   "metadata": {},
   "source": [
    "Tips:\n",
    "1. When you have a sum in the forward pass, that turns into a broadcasting in the backward pass. When you have a broadcasting in the forward pass, that indicates variable re-use so in the backward pass, it becomes a sum.\n",
    "2. Deriving the backward pass of a matrix multiplication:\n",
    "  - Dimensions need to work out, e.g. dx (derivative of loss w.r.t. x) must have same dimension as x\n",
    "  - Backward pass of a matrix multiplication is also a matrix multiplication, so transpose as necessary to get the required dimension\n",
    "3. Indexing:\n",
    "  - We have `emb = C[Xb]` in the forward pass\n",
    "  - `Xb` is (32,3), each element being a number that is used to index into `C`\n",
    "  - `emb` is (32,3,10), where for each element in `Xb`, we have the 10-dimensional embedding\n",
    "  - For indices `i`,`j`, `demb[i,j]` represents the gradients for each element of the embedding of the character corresponding to`X[i,j]`\n",
    "  - Need to accumulate gradients because rows will be reused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b11fb-8fb3-46db-b357-a05e0eca4edf",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "192ae4f9-c674-4620-933a-56a61c85a29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.335263729095459 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "078901ae-aac9-44ab-8287-349efe0bf83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | shape_match: True   | exact: False | approximate: True  | maxdiff: 6.984919309616089e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dlogits = F.softmax(logits, dim=1) - F.one_hot(Yb, num_classes=27)\n",
    "dlogits *= 1/n\n",
    "# dprobs = torch.zeros_like(logprobs)\n",
    "# dprobs[range(n),Yb] = 1.0 / (probs * n)\n",
    "# dnorm_logits = torch.full(norm_logits.shape, norm_logits.shape[0] - 1) * dprobs\n",
    "# -----------------\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b91823-c3e5-4f26-8ac3-c787627249f4",
   "metadata": {},
   "source": [
    "### Interpreting `dlogits`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd438111-7ad2-461d-b705-e5bc1a9d1dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13c4d0d50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64762690-3b2e-4fa1-b5db-ae58fb93af68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0681, 0.0923, 0.0189, 0.0520, 0.0209, 0.0827, 0.0228, 0.0354, 0.0179,\n",
       "        0.0308, 0.0369, 0.0354, 0.0361, 0.0288, 0.0352, 0.0135, 0.0089, 0.0194,\n",
       "        0.0166, 0.0556, 0.0475, 0.0202, 0.0250, 0.0770, 0.0572, 0.0253, 0.0199],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97c3c47b-c9b3-492f-bd8d-135eba2542a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0681,  0.0923,  0.0189,  0.0520,  0.0209,  0.0827,  0.0228,  0.0354,\n",
       "        -0.9821,  0.0308,  0.0369,  0.0354,  0.0361,  0.0288,  0.0352,  0.0135,\n",
       "         0.0089,  0.0194,  0.0166,  0.0556,  0.0475,  0.0202,  0.0250,  0.0770,\n",
       "         0.0572,  0.0253,  0.0199], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n # multiply n to cancel out the effect of averaging over examples, for interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "346f741f-0139-4108-a9e0-835b0d08d703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.3132e-10, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum() # == 0, barring floating point imprecision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d79ff-0f75-4813-9bed-7e895da232c7",
   "metadata": {},
   "source": [
    "Note that the gradient of the logits `dlogits` is positive in all directions, except in the index that pertains to the target, and `dlogits` sums to 0. We can interpret each element of `dlogits` as a force to the probabilities that pulls down on the incorrect characters, and pushes up the probability of the correct character. The amount of push and pull is exactly equalized. Think of this as pulling down the probability of an incorrect character by the amount in `dlogits` and pushing up the probability of the correct character by that same amount.\n",
    "\n",
    "The amount of force that we are applying is proportional to the probabilities (`logits`) that we obtained in the forward pass (i.e. how wrong we were). If `logits` was exactly correct, `dlogits` would be all zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9fc6a4-b932-436c-aac4-cf2725fdb219",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82fc8e73-eaca-4b04-8e02-9b3a995854d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fc324ea-f8e8-4f22-96f0-38b9b8ddd4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | shape_match: True   | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn = bngain * bnvar_inv / n * (n * dhpreact - dhpreact.sum(0) - n / (n - 1) * bnraw * (dhpreact * bnraw).sum(0))\n",
    "# -----------------\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78170899-c400-4696-8fa0-f1e9f3e731b2",
   "metadata": {},
   "source": [
    "A very very important caveat in the above exercise was dealing with backpropagation from scalars to vectors. For example, in the forward pass, `bnvar` affects all the individual elements of `bnraw` (variable reuse!), so in the backward pass, we actually need to sum over the gradients of all elements of `bnraw` with respect to `bnvar`.\n",
    "\n",
    "Also, note that `hprebn` is used to compute the mean, variance, and raw normalized output, so there is variable reuse here as well. (Similarly, the mean is used to compute variance and the raw normalized output.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c14c156-142c-45a8-b663-0ccf8eb318c4",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4883113f-60e5-4e4f-8199-f966317b9160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.4417\n",
      "  10000/ 200000: 2.4805\n",
      "  20000/ 200000: 2.2739\n",
      "  30000/ 200000: 2.5958\n",
      "  40000/ 200000: 2.4781\n",
      "  50000/ 200000: 2.5309\n",
      "  60000/ 200000: 1.7975\n",
      "  70000/ 200000: 1.9558\n",
      "  80000/ 200000: 2.1541\n",
      "  90000/ 200000: 2.6289\n",
      " 100000/ 200000: 2.1386\n",
      " 110000/ 200000: 1.8859\n",
      " 120000/ 200000: 2.1047\n",
      " 130000/ 200000: 2.3534\n",
      " 140000/ 200000: 2.2295\n",
      " 150000/ 200000: 2.5540\n",
      " 160000/ 200000: 2.0809\n",
      " 170000/ 200000: 2.1236\n",
      " 180000/ 200000: 2.1236\n",
      " 190000/ 200000: 2.2008\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647+10) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "    # kick off optimization\n",
    "    for i in range(max_steps):\n",
    "\n",
    "        # minibatch construct\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "        Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[Xb] # embed the characters into vectors\n",
    "        embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "        # Linear layer\n",
    "        hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "        # BatchNorm layer\n",
    "        # -------------------------------------------------------------\n",
    "        bnmean = hprebn.mean(0, keepdim=True)\n",
    "        bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "        bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "        bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "        # -------------------------------------------------------------\n",
    "        # Non-linearity\n",
    "        h = torch.tanh(hpreact) # hidden layer\n",
    "        logits = h @ W2 + b2 # output layer\n",
    "        loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        # loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "        # manual backprop! #swole_doge_meme\n",
    "        # -----------------\n",
    "        # YOUR CODE HERE :)\n",
    "        # softmax\n",
    "        dlogits = F.softmax(logits, dim=1) - F.one_hot(Yb, num_classes=vocab_size)\n",
    "        dlogits *= 1/n\n",
    "        # 2nd layer\n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0)\n",
    "        # tanh\n",
    "        dhpreact = dh * (1 - h**2)\n",
    "        # batchnorm\n",
    "        dbngain = (dhpreact * bnraw).sum(0, keepdim=True)\n",
    "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "        dhprebn = bngain * bnvar_inv / n * (n * dhpreact - dhpreact.sum(0) - n / (n - 1) * bnraw * (dhpreact * bnraw).sum(0))\n",
    "        # 1st layer\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "        # embedding\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "        for k in range(Xb.shape[0]):\n",
    "            for j in range(Xb.shape[1]):\n",
    "                ix = Xb[k,j]\n",
    "                dC[ix] += demb[k,j]\n",
    "\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "        # -----------------\n",
    "\n",
    "        # update\n",
    "        lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "            p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "        # track stats\n",
    "        if i % 10000 == 0: # print every once in a while\n",
    "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "        lossi.append(loss.log10().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f472ac9-2528-4909-ab33-dfe8a8051d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#     cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0224edb1-677b-445c-b9a3-1b6171857b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure the mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46105be7-a03c-4cd4-b3d8-42c272ac0a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0725810527801514\n",
      "val 2.1130170822143555\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x] # (N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ae1a25e-32d6-47c0-b867-4e5885fcb0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celia.\n",
      "moulcuriana.\n",
      "kayden.\n",
      "maimitta.\n",
      "nylandr.\n",
      "katar.\n",
      "samiyah.\n",
      "javer.\n",
      "gotis.\n",
      "moriella.\n",
      "kinzie.\n",
      "daren.\n",
      "emiless.\n",
      "suhakavion.\n",
      "ratlspihaniel.\n",
      "viah.\n",
      "ash.\n",
      "dedri.\n",
      "anell.\n",
      "pen.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        # forward pass\n",
    "        emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "        embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "        h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "        logits = h @ W2 + b2 # (N, vocab_size)\n",
    "        # sample\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
