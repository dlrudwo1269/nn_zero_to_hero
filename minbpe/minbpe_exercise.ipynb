{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "283909cc-84d2-4fb9-a7bb-de3aec7496c1",
   "metadata": {},
   "source": [
    "# exercise\n",
    "\n",
    "Build your own GPT-4 Tokenizer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d376cabc-26b8-474a-a4ce-574df82149a6",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1\n",
    "\n",
    "Write the `BasicTokenizer` class, with the following three core functions:\n",
    "\n",
    "- `def train(self, text, vocab_size, verbose=False)`\n",
    "- `def encode(self, text)`\n",
    "- `def decode(self, ids)`\n",
    "\n",
    "Train your tokenizer on whatever text you like and visualize the merged tokens. Do they look reasonable? One default test you may wish to use is the text file `tests/taylorswift.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0400f75-d63b-402e-9b03-cec71a13ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/minbpe/master/tests/taylorswift.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc6771b4-f517-4411-aa6e-c88d638198ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids, counts=None):\n",
    "    \"\"\" Count the frequency of each consecutive pair of tokens. Update counts in-place if provided \"\"\"\n",
    "    pair_frequency = counts if counts is not None else defaultdict(int) # (int, int) -> int\n",
    "    for id1, id2 in zip(ids, ids[1:]):\n",
    "        pair_frequency[(id1, id2)] += 1\n",
    "    return pair_frequency\n",
    "\n",
    "def merge(ids, pair, new_tok):\n",
    "    \"\"\" Given a list of tokens, a pair to merge, and a new token, replace all occurrences of pair with new_tok. \"\"\"\n",
    "    new_ids = []\n",
    "    idx = 0\n",
    "    while idx < len(ids):\n",
    "        if idx != len(ids) - 1 and (ids[idx], ids[idx+1]) == pair:\n",
    "            new_ids.append(new_tok)\n",
    "            idx += 2\n",
    "        else:\n",
    "            new_ids.append(ids[idx])\n",
    "            idx += 1\n",
    "    return new_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08cee4a5-3d7f-41c3-b52f-1effe949f7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (101, 32) -> 256\n",
      "Merging (44, 32) -> 257\n",
      "Merging (100, 32) -> 258\n",
      "Merging (46, 32) -> 259\n",
      "Merging (114, 32) -> 260\n",
      "Merging (50, 48) -> 261\n",
      "Merging (115, 32) -> 262\n",
      "Merging (105, 110) -> 263\n",
      "Merging (111, 110) -> 264\n",
      "Merging (114, 105) -> 265\n",
      "Merging (116, 32) -> 266\n",
      "Merging (116, 104) -> 267\n",
      "Merging (101, 258) -> 268\n",
      "Merging (257, 261) -> 269\n",
      "Merging (97, 110) -> 270\n",
      "Merging (97, 114) -> 271\n",
      "Merging (101, 260) -> 272\n",
      "Merging (121, 32) -> 273\n",
      "Merging (97, 108) -> 274\n",
      "Merging (267, 256) -> 275\n",
      "Merging (118, 268) -> 276\n",
      "Merging (119, 105) -> 277\n",
      "Merging (101, 114) -> 278\n",
      "Merging (264, 32) -> 279\n",
      "Merging (277, 102) -> 280\n",
      "Merging (82, 101) -> 281\n",
      "Merging (83, 280) -> 282\n",
      "Merging (111, 260) -> 283\n",
      "Merging (99, 104) -> 284\n",
      "Merging (269, 49) -> 285\n",
      "Merging (111, 109) -> 286\n",
      "Merging (98, 272) -> 287\n",
      "Merging (32, 275) -> 288\n",
      "Merging (97, 121) -> 289\n",
      "Merging (101, 110) -> 290\n",
      "Merging (111, 114) -> 291\n",
      "Merging (274, 32) -> 292\n",
      "Merging (101, 109) -> 293\n",
      "Merging (46, 10) -> 294\n",
      "Merging (265, 101) -> 295\n",
      "Merging (263, 103) -> 296\n",
      "Merging (269, 50) -> 297\n",
      "Merging (116, 105) -> 298\n",
      "Merging (289, 108) -> 299\n",
      "Merging (34, 259) -> 300\n",
      "Merging (108, 108) -> 301\n",
      "Merging (84, 299) -> 302\n",
      "Merging (116, 295) -> 303\n",
      "Merging (294, 32) -> 304\n",
      "Merging (116, 111) -> 305\n",
      "Merging (259, 281) -> 306\n",
      "Merging (306, 303) -> 307\n",
      "Merging (307, 276) -> 308\n",
      "Merging (302, 283) -> 309\n",
      "Merging (101, 115) -> 310\n",
      "Merging (309, 282) -> 311\n",
      "Merging (117, 115) -> 312\n",
      "Merging (114, 286) -> 313\n",
      "Merging (293, 287) -> 314\n",
      "Merging (41, 259) -> 315\n",
      "Merging (65, 114) -> 316\n",
      "Merging (102, 313) -> 317\n",
      "Merging (315, 34) -> 318\n",
      "Merging (270, 258) -> 319\n",
      "Merging (114, 101) -> 320\n",
      "Merging (111, 117) -> 321\n",
      "Merging (111, 265) -> 322\n",
      "Merging (111, 102) -> 323\n",
      "Merging (103, 263) -> 324\n",
      "Merging (296, 32) -> 325\n",
      "Merging (284, 105) -> 326\n",
      "Merging (93, 32) -> 327\n",
      "Merging (324, 292) -> 328\n",
      "Merging (317, 288) -> 329\n",
      "Merging (322, 328) -> 330\n",
      "Merging (104, 256) -> 331\n",
      "Merging (316, 326) -> 332\n",
      "Merging (332, 276) -> 333\n",
      "Merging (329, 330) -> 334\n",
      "Merging (333, 334) -> 335\n",
      "Merging (335, 279) -> 336\n",
      "Merging (259, 336) -> 337\n",
      "Merging (97, 32) -> 338\n",
      "Merging (115, 116) -> 339\n",
      "Merging (105, 99) -> 340\n",
      "Merging (46, 91) -> 341\n",
      "Merging (101, 99) -> 342\n",
      "Merging (105, 301) -> 343\n",
      "Merging (39, 262) -> 344\n",
      "Merging (311, 266) -> 345\n",
      "Merging (111, 118) -> 346\n",
      "Merging (97, 116) -> 347\n",
      "Merging (97, 262) -> 348\n",
      "Merging (101, 262) -> 349\n",
      "Merging (74, 117) -> 350\n",
      "Merging (323, 32) -> 351\n",
      "Merging (305, 32) -> 352\n",
      "Merging (117, 109) -> 353\n",
      "Merging (84, 331) -> 354\n",
      "Merging (271, 100) -> 355\n",
      "Merging (263, 32) -> 356\n",
      "Merging (270, 32) -> 357\n",
      "Merging (101, 108) -> 358\n",
      "Merging (297, 51) -> 359\n",
      "Merging (271, 273) -> 360\n",
      "Merging (267, 32) -> 361\n",
      "Merging (97, 109) -> 362\n",
      "Merging (108, 273) -> 363\n",
      "Merging (111, 112) -> 364\n",
      "Merging (311, 116) -> 365\n",
      "Merging (116, 114) -> 366\n",
      "Merging (105, 115) -> 367\n",
      "Merging (104, 272) -> 368\n",
      "Merging (111, 32) -> 369\n",
      "Merging (117, 360) -> 370\n",
      "Merging (78, 346) -> 371\n",
      "Merging (312, 340) -> 372\n",
      "Merging (371, 314) -> 373\n",
      "Merging (101, 119) -> 374\n",
      "Merging (97, 266) -> 375\n",
      "Merging (108, 32) -> 376\n",
      "Merging (58, 32) -> 377\n",
      "Merging (98, 111) -> 378\n",
      "Merging (282, 266) -> 379\n",
      "Merging (68, 342) -> 380\n",
      "Merging (105, 116) -> 381\n",
      "Merging (105, 103) -> 382\n",
      "Merging (66, 343) -> 383\n",
      "Merging (49, 48) -> 384\n",
      "Merging (97, 115) -> 385\n",
      "Merging (264, 103) -> 386\n",
      "Merging (79, 99) -> 387\n",
      "Merging (97, 298) -> 388\n",
      "Merging (83, 116) -> 389\n",
      "Merging (387, 305) -> 390\n",
      "Merging (390, 287) -> 391\n",
      "Merging (97, 99) -> 392\n",
      "Merging (111, 119) -> 393\n",
      "Merging (380, 314) -> 394\n",
      "Merging (383, 378) -> 395\n",
      "Merging (97, 100) -> 396\n",
      "Merging (108, 101) -> 397\n",
      "Merging (117, 114) -> 398\n",
      "Merging (102, 283) -> 399\n",
      "Merging (32, 40) -> 400\n",
      "Merging (297, 50) -> 401\n",
      "Merging (117, 103) -> 402\n",
      "Merging (284, 32) -> 403\n",
      "Merging (115, 266) -> 404\n",
      "Merging (321, 110) -> 405\n",
      "Merging (98, 353) -> 406\n",
      "Merging (111, 108) -> 407\n",
      "Merging (312, 266) -> 408\n",
      "Merging (101, 98) -> 409\n",
      "Merging (77, 97) -> 410\n",
      "Merging (350, 363) -> 411\n",
      "Merging (318, 345) -> 412\n",
      "Merging (107, 32) -> 413\n",
      "Merging (278, 115) -> 414\n",
      "Merging (93, 91) -> 415\n",
      "Merging (65, 402) -> 416\n",
      "Merging (416, 408) -> 417\n",
      "Merging (105, 100) -> 418\n",
      "Merging (297, 49) -> 419\n",
      "Merging (109, 101) -> 420\n",
      "Merging (101, 112) -> 421\n",
      "Merging (261, 49) -> 422\n",
      "Merging (50, 51) -> 423\n",
      "Merging (285, 50) -> 424\n",
      "Merging (101, 271) -> 425\n",
      "Merging (269, 261) -> 426\n",
      "Merging (73, 110) -> 427\n",
      "Merging (102, 105) -> 428\n",
      "Merging (110, 256) -> 429\n",
      "Merging (395, 355) -> 430\n",
      "Merging (265, 116) -> 431\n",
      "Merging (104, 105) -> 432\n",
      "Merging (372, 32) -> 433\n",
      "Merging (304, 34) -> 434\n",
      "Merging (78, 374) -> 435\n",
      "Merging (100, 105) -> 436\n",
      "Merging (65, 112) -> 437\n",
      "Merging (285, 57) -> 438\n",
      "Merging (114, 111) -> 439\n",
      "Merging (39, 32) -> 440\n",
      "Merging (115, 257) -> 441\n",
      "Merging (350, 429) -> 442\n",
      "Merging (323, 288) -> 443\n",
      "Merging (99, 291) -> 444\n",
      "Merging (50, 49) -> 445\n",
      "Merging (49, 57) -> 446\n",
      "Merging (105, 109) -> 447\n",
      "Merging (290, 32) -> 448\n",
      "Merging (409, 114) -> 449\n",
      "Merging (290, 116) -> 450\n",
      "Merging (111, 301) -> 451\n",
      "Merging (77, 271) -> 452\n",
      "Merging (265, 99) -> 453\n",
      "Merging (277, 361) -> 454\n",
      "Merging (44, 91) -> 455\n",
      "Merging (70, 449) -> 456\n",
      "Merging (456, 370) -> 457\n",
      "Merging (365, 344) -> 458\n",
      "Merging (300, 430) -> 459\n",
      "Merging (101, 97) -> 460\n",
      "Merging (285, 54) -> 461\n",
      "Merging (421, 116) -> 462\n",
      "Merging (410, 273) -> 463\n",
      "Merging (285, 53) -> 464\n",
      "Merging (437, 265) -> 465\n",
      "Merging (465, 376) -> 466\n",
      "Merging (108, 256) -> 467\n",
      "Merging (65, 119) -> 468\n",
      "Merging (388, 264) -> 469\n",
      "Merging (83, 462) -> 470\n",
      "Merging (470, 314) -> 471\n",
      "Merging (114, 97) -> 472\n",
      "Merging (274, 406) -> 473\n",
      "Merging (67, 104) -> 474\n",
      "Merging (118, 256) -> 475\n",
      "Merging (310, 266) -> 476\n",
      "Merging (74, 270) -> 477\n",
      "Merging (50, 50) -> 478\n",
      "Merging (477, 370) -> 479\n",
      "Merging (405, 366) -> 480\n",
      "Merging (382, 104) -> 481\n",
      "Merging (300, 354) -> 482\n",
      "Merging (359, 304) -> 483\n",
      "Merging (49, 51) -> 484\n",
      "Merging (65, 108) -> 485\n",
      "Merging (101, 116) -> 486\n",
      "Merging (310, 115) -> 487\n",
      "Merging (452, 403) -> 488\n",
      "Merging (117, 116) -> 489\n",
      "Merging (119, 431) -> 490\n",
      "Merging (108, 111) -> 491\n",
      "Merging (115, 386) -> 492\n",
      "Merging (226, 128) -> 493\n",
      "Merging (271, 258) -> 494\n",
      "Merging (48, 32) -> 495\n",
      "Merging (117, 108) -> 496\n",
      "Merging (50, 52) -> 497\n",
      "Merging (105, 262) -> 498\n",
      "Merging (298, 99) -> 499\n",
      "Merging (97, 103) -> 500\n",
      "Merging (34, 32) -> 501\n",
      "Merging (65, 110) -> 502\n",
      "Merging (49, 56) -> 503\n",
      "Merging (102, 291) -> 504\n",
      "Merging (480, 273) -> 505\n",
      "Merging (65, 420) -> 506\n",
      "Merging (506, 453) -> 507\n",
      "Merging (32, 84) -> 508\n",
      "Merging (115, 296) -> 509\n",
      "Merging (119, 348) -> 510\n",
      "Merging (49, 50) -> 511\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class BasicTokenizer:\n",
    "    INITIAL_VOCAB_SIZE = 256\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {i : i.to_bytes() for i in range(self.INITIAL_VOCAB_SIZE)} # int -> bytes\n",
    "        self.merges = {} # (int, int) -> int\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        \"\"\" Given training data as a single string, perform BPE merges until we reach vocab_size tokens,\n",
    "        and store the information in self.vocab and self.merges.\n",
    "        \"\"\"\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        for i in range(vocab_size - self.INITIAL_VOCAB_SIZE):\n",
    "            # find pair to merge\n",
    "            pair_frequency = get_stats(ids)\n",
    "            most_frequent_pair = max(pair_frequency, key=lambda p: pair_frequency.get(p, float(\"-inf\")))\n",
    "        \n",
    "            # mint new token and add it to our vocab\n",
    "            next_token = self.INITIAL_VOCAB_SIZE + i\n",
    "            self.vocab[next_token] = self.vocab[most_frequent_pair[0]] + self.vocab[most_frequent_pair[1]]\n",
    "            self.merges[most_frequent_pair] = next_token\n",
    "            if verbose:\n",
    "                print(f\"Merging {most_frequent_pair} -> {next_token}\")\n",
    "            # perform merge on our input for the next round\n",
    "            ids = merge(ids, most_frequent_pair, next_token)\n",
    "            \n",
    "    def encode(self, text):\n",
    "        \"\"\" text -> bytes using self.vocab and self.merges \"\"\"\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        # perform all merges\n",
    "        while len(ids) > 1:\n",
    "            pair_frequency = get_stats(ids)\n",
    "            # since a new token may be merged with a subsequent token, we must process lowest index tokens first\n",
    "            pair_to_merge = min(pair_frequency, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair_to_merge not in self.merges:\n",
    "                break\n",
    "            ids = merge(ids, pair_to_merge, self.merges[pair_to_merge])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\" bytes -> text using self.vocab and self.merges \"\"\"\n",
    "        return b\"\".join([self.vocab[tok] for tok in ids]).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "text = open(\"taylorswift.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "    \n",
    "tok = BasicTokenizer()\n",
    "tok.train(text, 512, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d06fd23c-b433-40b9-885e-150a2a93e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 301, 369, 116, 299, 283, 115, 280, 116]\n",
      "hello taylor swift\n"
     ]
    }
   ],
   "source": [
    "print(tok.encode(\"hello taylor swift\"))\n",
    "print(tok.decode(tok.encode(\"hello taylor swift\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f9240a-c5fd-418d-b811-f495e983ccc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{256: b'e ', 257: b', ', 258: b'd ', 259: b'. ', 260: b'r ', 261: b'20', 262: b's ', 263: b'in', 264: b'on', 265: b'ri', 266: b't ', 267: b'th', 268: b'ed ', 269: b', 20', 270: b'an', 271: b'ar', 272: b'er ', 273: b'y ', 274: b'al', 275: b'the ', 276: b'ved ', 277: b'wi', 278: b'er', 279: b'on ', 280: b'wif', 281: b'Re', 282: b'Swif', 283: b'or ', 284: b'ch', 285: b', 201', 286: b'om', 287: b'ber ', 288: b' the ', 289: b'ay', 290: b'en', 291: b'or', 292: b'al ', 293: b'em', 294: b'.\\n', 295: b'rie', 296: b'ing', 297: b', 202', 298: b'ti', 299: b'ayl', 300: b'\". ', 301: b'll', 302: b'Tayl', 303: b'trie', 304: b'.\\n ', 305: b'to', 306: b'. Re', 307: b'. Retrie', 308: b'. Retrieved ', 309: b'Taylor ', 310: b'es', 311: b'Taylor Swif', 312: b'us', 313: b'rom', 314: b'ember ', 315: b'). ', 316: b'Ar', 317: b'from', 318: b'). \"', 319: b'and ', 320: b're', 321: b'ou', 322: b'ori', 323: b'of', 324: b'gin', 325: b'ing ', 326: b'chi', 327: b'] ', 328: b'ginal ', 329: b'from the ', 330: b'original ', 331: b'he ', 332: b'Archi', 333: b'Archived ', 334: b'from the original ', 335: b'Archived from the original ', 336: b'Archived from the original on ', 337: b'. Archived from the original on ', 338: b'a ', 339: b'st', 340: b'ic', 341: b'.[', 342: b'ec', 343: b'ill', 344: b\"'s \", 345: b'Taylor Swift ', 346: b'ov', 347: b'at', 348: b'as ', 349: b'es ', 350: b'Ju', 351: b'of ', 352: b'to ', 353: b'um', 354: b'The ', 355: b'ard', 356: b'in ', 357: b'an ', 358: b'el', 359: b', 2023', 360: b'ary ', 361: b'th ', 362: b'am', 363: b'ly ', 364: b'op', 365: b'Taylor Swift', 366: b'tr', 367: b'is', 368: b'her ', 369: b'o ', 370: b'uary ', 371: b'Nov', 372: b'usic', 373: b'November ', 374: b'ew', 375: b'at ', 376: b'l ', 377: b': ', 378: b'bo', 379: b'Swift ', 380: b'Dec', 381: b'it', 382: b'ig', 383: b'Bill', 384: b'10', 385: b'as', 386: b'ong', 387: b'Oc', 388: b'ati', 389: b'St', 390: b'Octo', 391: b'October ', 392: b'ac', 393: b'ow', 394: b'December ', 395: b'Billbo', 396: b'ad', 397: b'le', 398: b'ur', 399: b'for ', 400: b' (', 401: b', 2022', 402: b'ug', 403: b'ch ', 404: b'st ', 405: b'oun', 406: b'bum', 407: b'ol', 408: b'ust ', 409: b'eb', 410: b'Ma', 411: b'July ', 412: b'). \"Taylor Swift ', 413: b'k ', 414: b'ers', 415: b'][', 416: b'Aug', 417: b'August ', 418: b'id', 419: b', 2021', 420: b'me', 421: b'ep', 422: b'201', 423: b'23', 424: b', 2012', 425: b'ear', 426: b', 2020', 427: b'In', 428: b'fi', 429: b'ne ', 430: b'Billboard', 431: b'rit', 432: b'hi', 433: b'usic ', 434: b'.\\n \"', 435: b'New', 436: b'di', 437: b'Ap', 438: b', 2019', 439: b'ro', 440: b\"' \", 441: b's, ', 442: b'June ', 443: b'of the ', 444: b'cor', 445: b'21', 446: b'19', 447: b'im', 448: b'en ', 449: b'ebr', 450: b'ent', 451: b'oll', 452: b'Mar', 453: b'ric', 454: b'with ', 455: b',[', 456: b'Febr', 457: b'February ', 458: b\"Taylor Swift's \", 459: b'\". Billboard', 460: b'ea', 461: b', 2016', 462: b'ept', 463: b'May ', 464: b', 2015', 465: b'Apri', 466: b'April ', 467: b'le ', 468: b'Aw', 469: b'ation', 470: b'Sept', 471: b'September ', 472: b'ra', 473: b'album', 474: b'Ch', 475: b've ', 476: b'est ', 477: b'Jan', 478: b'22', 479: b'January ', 480: b'ountr', 481: b'igh', 482: b'\". The ', 483: b', 2023.\\n ', 484: b'13', 485: b'Al', 486: b'et', 487: b'ess', 488: b'March ', 489: b'ut', 490: b'writ', 491: b'lo', 492: b'song', 493: b'\\xe2\\x80', 494: b'ard ', 495: b'0 ', 496: b'ul', 497: b'24', 498: b'is ', 499: b'tic', 500: b'ag', 501: b'\" ', 502: b'An', 503: b'18', 504: b'for', 505: b'ountry ', 506: b'Ame', 507: b'Americ', 508: b' T', 509: b'sing', 510: b'was ', 511: b'12'}\n"
     ]
    }
   ],
   "source": [
    "print({k:v for k, v in tok.vocab.items() if k > 255}) # print the merged tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c096c08-8039-4e4d-8327-b4ce5f3c6c2b",
   "metadata": {},
   "source": [
    "Note we have some cross-category merging without the regex forced split, such as `337: b'. Archived from the original on '` or `412: b'). \"Taylor Swift '`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a7992-19b8-48ae-9aca-326e9c45d48f",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Convert you `BasicTokenizer` into a `RegexTokenizer`, which takes a regex pattern and splits the text exactly as GPT-4 would. Process the parts separately as before, then concatenate the results. Retrain your tokenizer and compare the results before and after. You should see that you will now have no tokens that go across categories (numbers, letters, punctuation, more than one whitespace). Use the GPT-4 pattern:\n",
    "\n",
    "```\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ffa3cc-10f5-449a-8cbc-c9d2bfcab074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (101, 114) -> 256\n",
      "Merging (50, 48) -> 257\n",
      "Merging (111, 114) -> 258\n",
      "Merging (105, 110) -> 259\n",
      "Merging (101, 100) -> 260\n",
      "Merging (32, 116) -> 261\n",
      "Merging (111, 110) -> 262\n",
      "Merging (104, 101) -> 263\n",
      "Merging (32, 83) -> 264\n",
      "Merging (97, 114) -> 265\n",
      "Merging (97, 110) -> 266\n",
      "Merging (32, 65) -> 267\n",
      "Merging (261, 263) -> 268\n",
      "Merging (97, 108) -> 269\n",
      "Merging (114, 105) -> 270\n",
      "Merging (118, 260) -> 271\n",
      "Merging (115, 116) -> 272\n",
      "Merging (119, 105) -> 273\n",
      "Merging (32, 82) -> 274\n",
      "Merging (257, 49) -> 275\n",
      "Merging (32, 102) -> 276\n",
      "Merging (257, 50) -> 277\n",
      "Merging (32, 84) -> 278\n",
      "Merging (102, 116) -> 279\n",
      "Merging (97, 121) -> 280\n",
      "Merging (32, 34) -> 281\n",
      "Merging (273, 279) -> 282\n",
      "Merging (101, 116) -> 283\n",
      "Merging (264, 282) -> 284\n",
      "Merging (99, 104) -> 285\n",
      "Merging (98, 256) -> 286\n",
      "Merging (97, 116) -> 287\n",
      "Merging (111, 109) -> 288\n",
      "Merging (101, 115) -> 289\n",
      "Merging (101, 110) -> 290\n",
      "Merging (101, 109) -> 291\n",
      "Merging (34, 46) -> 292\n",
      "Merging (32, 40) -> 293\n",
      "Merging (46, 10) -> 294\n",
      "Merging (259, 103) -> 295\n",
      "Merging (108, 258) -> 296\n",
      "Merging (32, 77) -> 297\n",
      "Merging (105, 103) -> 298\n",
      "Merging (32, 262) -> 299\n",
      "Merging (280, 296) -> 300\n",
      "Merging (108, 108) -> 301\n",
      "Merging (270, 101) -> 302\n",
      "Merging (274, 283) -> 303\n",
      "Merging (303, 302) -> 304\n",
      "Merging (304, 271) -> 305\n",
      "Merging (32, 115) -> 306\n",
      "Merging (105, 99) -> 307\n",
      "Merging (266, 100) -> 308\n",
      "Merging (111, 117) -> 309\n",
      "Merging (101, 99) -> 310\n",
      "Merging (32, 97) -> 311\n",
      "Merging (41, 46) -> 312\n",
      "Merging (114, 288) -> 313\n",
      "Merging (32, 66) -> 314\n",
      "Merging (291, 286) -> 315\n",
      "Merging (32, 111) -> 316\n",
      "Merging (276, 313) -> 317\n",
      "Merging (267, 114) -> 318\n",
      "Merging (32, 308) -> 319\n",
      "Merging (32, 67) -> 320\n",
      "Merging (32, 78) -> 321\n",
      "Merging (32, 258) -> 322\n",
      "Merging (285, 105) -> 323\n",
      "Merging (32, 74) -> 324\n",
      "Merging (259, 269) -> 325\n",
      "Merging (322, 298) -> 326\n",
      "Merging (326, 325) -> 327\n",
      "Merging (318, 323) -> 328\n",
      "Merging (328, 271) -> 329\n",
      "Merging (316, 102) -> 330\n",
      "Merging (32, 104) -> 331\n",
      "Merging (32, 259) -> 332\n",
      "Merging (114, 101) -> 333\n",
      "Merging (84, 300) -> 334\n",
      "Merging (105, 116) -> 335\n",
      "Merging (97, 115) -> 336\n",
      "Merging (32, 112) -> 337\n",
      "Merging (105, 262) -> 338\n",
      "Merging (32, 68) -> 339\n",
      "Merging (32, 119) -> 340\n",
      "Merging (265, 100) -> 341\n",
      "Merging (105, 301) -> 342\n",
      "Merging (39, 115) -> 343\n",
      "Merging (32, 109) -> 344\n",
      "Merging (32, 70) -> 345\n",
      "Merging (32, 87) -> 346\n",
      "Merging (108, 101) -> 347\n",
      "Merging (261, 111) -> 348\n",
      "Merging (32, 99) -> 349\n",
      "Merging (46, 91) -> 350\n",
      "Merging (111, 118) -> 351\n",
      "Merging (108, 121) -> 352\n",
      "Merging (117, 115) -> 353\n",
      "Merging (32, 72) -> 354\n",
      "Merging (105, 115) -> 355\n",
      "Merging (32, 80) -> 356\n",
      "Merging (116, 104) -> 357\n",
      "Merging (99, 116) -> 358\n",
      "Merging (117, 109) -> 359\n",
      "Merging (32, 98) -> 360\n",
      "Merging (32, 71) -> 361\n",
      "Merging (265, 121) -> 362\n",
      "Merging (32, 73) -> 363\n",
      "Merging (278, 300) -> 364\n",
      "Merging (105, 272) -> 365\n",
      "Merging (32, 100) -> 366\n",
      "Merging (97, 109) -> 367\n",
      "Merging (32, 79) -> 368\n",
      "Merging (111, 112) -> 369\n",
      "Merging (324, 117) -> 370\n",
      "Merging (331, 256) -> 371\n",
      "Merging (32, 76) -> 372\n",
      "Merging (117, 272) -> 373\n",
      "Merging (97, 100) -> 374\n",
      "Merging (278, 263) -> 375\n",
      "Merging (117, 362) -> 376\n",
      "Merging (290, 116) -> 377\n",
      "Merging (353, 307) -> 378\n",
      "Merging (351, 315) -> 379\n",
      "Merging (101, 119) -> 380\n",
      "Merging (256, 115) -> 381\n",
      "Merging (265, 116) -> 382\n",
      "Merging (105, 100) -> 383\n",
      "Merging (32, 69) -> 384\n",
      "Merging (101, 108) -> 385\n",
      "Merging (262, 103) -> 386\n",
      "Merging (105, 109) -> 387\n",
      "Merging (111, 286) -> 388\n",
      "Merging (101, 112) -> 389\n",
      "Merging (276, 258) -> 390\n",
      "Merging (358, 388) -> 391\n",
      "Merging (98, 111) -> 392\n",
      "Merging (314, 342) -> 393\n",
      "Merging (310, 315) -> 394\n",
      "Merging (264, 116) -> 395\n",
      "Merging (392, 341) -> 396\n",
      "Merging (111, 119) -> 397\n",
      "Merging (117, 103) -> 398\n",
      "Merging (111, 116) -> 399\n",
      "Merging (393, 396) -> 400\n",
      "Merging (49, 48) -> 401\n",
      "Merging (110, 116) -> 402\n",
      "Merging (110, 101) -> 403\n",
      "Merging (101, 265) -> 404\n",
      "Merging (309, 114) -> 405\n",
      "Merging (270, 116) -> 406\n",
      "Merging (105, 114) -> 407\n",
      "Merging (98, 359) -> 408\n",
      "Merging (117, 114) -> 409\n",
      "Merging (287, 338) -> 410\n",
      "Merging (257, 48) -> 411\n",
      "Merging (111, 108) -> 412\n",
      "Merging (289, 115) -> 413\n",
      "Merging (32, 114) -> 414\n",
      "Merging (93, 91) -> 415\n",
      "Merging (398, 373) -> 416\n",
      "Merging (32, 86) -> 417\n",
      "Merging (32, 39) -> 418\n",
      "Merging (101, 98) -> 419\n",
      "Merging (32, 269) -> 420\n",
      "Merging (105, 118) -> 421\n",
      "Merging (32, 89) -> 422\n",
      "Merging (117, 116) -> 423\n",
      "Merging (32, 273) -> 424\n",
      "Merging (114, 121) -> 425\n",
      "Merging (101, 272) -> 426\n",
      "Merging (424, 357) -> 427\n",
      "Merging (114, 97) -> 428\n",
      "Merging (339, 394) -> 429\n",
      "Merging (321, 379) -> 430\n",
      "Merging (32, 287) -> 431\n",
      "Merging (32, 333) -> 432\n",
      "Merging (370, 352) -> 433\n",
      "Merging (261, 104) -> 434\n",
      "Merging (32, 110) -> 435\n",
      "Merging (267, 416) -> 436\n",
      "Merging (258, 100) -> 437\n",
      "Merging (119, 341) -> 438\n",
      "Merging (49, 57) -> 439\n",
      "Merging (32, 75) -> 440\n",
      "Merging (368, 391) -> 441\n",
      "Merging (32, 108) -> 442\n",
      "Merging (111, 301) -> 443\n",
      "Merging (112, 270) -> 444\n",
      "Merging (117, 108) -> 445\n",
      "Merging (389, 116) -> 446\n",
      "Merging (297, 378) -> 447\n",
      "Merging (32, 85) -> 448\n",
      "Merging (111, 115) -> 449\n",
      "Merging (419, 114) -> 450\n",
      "Merging (311, 115) -> 451\n",
      "Merging (110, 100) -> 452\n",
      "Merging (44, 91) -> 453\n",
      "Merging (450, 376) -> 454\n",
      "Merging (321, 380) -> 455\n",
      "Merging (32, 272) -> 456\n",
      "Merging (111, 100) -> 457\n",
      "Merging (97, 107) -> 458\n",
      "Merging (444, 108) -> 459\n",
      "Merging (309, 402) -> 460\n",
      "Merging (320, 104) -> 461\n",
      "Merging (99, 101) -> 462\n",
      "Merging (446, 315) -> 463\n",
      "Merging (420, 408) -> 464\n",
      "Merging (105, 108) -> 465\n",
      "Merging (267, 438) -> 466\n",
      "Merging (310, 437) -> 467\n",
      "Merging (266, 376) -> 468\n",
      "Merging (49, 51) -> 469\n",
      "Merging (460, 425) -> 470\n",
      "Merging (109, 256) -> 471\n",
      "Merging (262, 101) -> 472\n",
      "Merging (32, 103) -> 473\n",
      "Merging (50, 49) -> 474\n",
      "Merging (265, 285) -> 475\n",
      "Merging (111, 99) -> 476\n",
      "Merging (310, 116) -> 477\n",
      "Merging (105, 97) -> 478\n",
      "Merging (118, 256) -> 479\n",
      "Merging (226, 128) -> 480\n",
      "Merging (264, 263) -> 481\n",
      "Merging (297, 280) -> 482\n",
      "Merging (267, 108) -> 483\n",
      "Merging (50, 51) -> 484\n",
      "Merging (370, 403) -> 485\n",
      "Merging (344, 378) -> 486\n",
      "Merging (49, 56) -> 487\n",
      "Merging (278, 387) -> 488\n",
      "Merging (421, 101) -> 489\n",
      "Merging (345, 454) -> 490\n",
      "Merging (101, 101) -> 491\n",
      "Merging (105, 266) -> 492\n",
      "Merging (50, 52) -> 493\n",
      "Merging (471, 307) -> 494\n",
      "Merging (49, 50) -> 495\n",
      "Merging (264, 463) -> 496\n",
      "Merging (306, 295) -> 497\n",
      "Merging (32, 118) -> 498\n",
      "Merging (340, 336) -> 499\n",
      "Merging (267, 459) -> 500\n",
      "Merging (49, 55) -> 501\n",
      "Merging (50, 50) -> 502\n",
      "Merging (111, 103) -> 503\n",
      "Merging (119, 406) -> 504\n",
      "Merging (78, 379) -> 505\n",
      "Merging (306, 386) -> 506\n",
      "Merging (49, 52) -> 507\n",
      "Merging (298, 104) -> 508\n",
      "Merging (108, 100) -> 509\n",
      "Merging (93, 10) -> 510\n",
      "Merging (306, 263) -> 511\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import regex as re\n",
    "\n",
    "class RegexTokenizer:\n",
    "    INITIAL_VOCAB_SIZE = 256\n",
    "    GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {i : i.to_bytes() for i in range(self.INITIAL_VOCAB_SIZE)} # int -> bytes\n",
    "        self.merges = {} # (int, int) -> int\n",
    "    \n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        \"\"\" Given training data as a single string, perform BPE merges until we reach vocab_size tokens,\n",
    "        and store the information in self.vocab and self.merges.\n",
    "        \"\"\"\n",
    "        chunks = re.findall(self.GPT4_SPLIT_PATTERN, text)\n",
    "        ids = [list(chunk.encode(\"utf-8\")) for chunk in chunks]\n",
    "        for i in range(vocab_size - self.INITIAL_VOCAB_SIZE):\n",
    "            # find most common pair across all chunks\n",
    "            pair_frequency = defaultdict(int)\n",
    "            for ids_chunk in ids:\n",
    "                get_stats(ids_chunk, pair_frequency) # update in-place\n",
    "            most_frequent_pair = max(pair_frequency, key=lambda p: pair_frequency.get(p, float(\"-inf\")))\n",
    "\n",
    "            # mint new token and add it to our vocab\n",
    "            next_token = self.INITIAL_VOCAB_SIZE + i\n",
    "            self.vocab[next_token] = self.vocab[most_frequent_pair[0]] + self.vocab[most_frequent_pair[1]]\n",
    "            self.merges[most_frequent_pair] = next_token\n",
    "            if verbose:\n",
    "                print(f\"Merging {most_frequent_pair} -> {next_token}\")\n",
    "            # perform merge on our input for the next round\n",
    "            ids = [merge(ids_chunk, most_frequent_pair, next_token) for ids_chunk in ids]\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\" text -> bytes using self.vocab and self.merges \"\"\"\n",
    "        ids = list(text.encode(\"utf-8\"))\n",
    "        # perform all merges\n",
    "        while len(ids) > 1:\n",
    "            pair_frequency = get_stats(ids)\n",
    "            # since a new token may be merged with a subsequent token, we must process lowest index tokens first\n",
    "            pair_to_merge = min(pair_frequency, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair_to_merge not in self.merges:\n",
    "                break\n",
    "            ids = merge(ids, pair_to_merge, self.merges[pair_to_merge])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\" bytes -> text using self.vocab and self.merges \"\"\"\n",
    "        return b\"\".join([self.vocab[tok] for tok in ids]).decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "\n",
    "text = open(\"taylorswift.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "    \n",
    "tok = RegexTokenizer()\n",
    "tok.train(text, 512, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1182fe25-13c5-4699-b3e2-c698f90b5833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 329, 317, 268, 327, 299, 32]\n",
      ". Archived from the original on \n"
     ]
    }
   ],
   "source": [
    "print(tok.encode(\". Archived from the original on \"))\n",
    "print(tok.decode(tok.encode(\". Archived from the original on \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31607f9d-c1b2-4099-8322-358ac85af792",
   "metadata": {},
   "source": [
    "We can see that the string `\". Archived from the original on \"` used to be a single token in the `BasicTokenizer`, but now it is broken up into multiple tokens due to the forced split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cdcb1c3-c86b-4122-a9c2-512f1d836516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(text == tok.decode(tok.encode(text))) # sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee49a0d-0275-436a-8d0f-192a4240565c",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3\n",
    "\n",
    "You're now ready to load the merges from the GPT-4 tokenizer and show that your tokenizer produces the identical results for both `encode` and `decode`, matching [tiktoken](https://github.com/openai/tiktoken).\n",
    "\n",
    "```\n",
    "# match this\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "ids = enc.encode(\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\")\n",
    "text = enc.decode(ids) # get the same text back\n",
    "```\n",
    "\n",
    "Unfortunately, you will run into two issues:\n",
    "\n",
    "1. It is not trivial to recover the raw merges from the GPT-4 tokenizer. You can easily recover what we call `vocab` here, and what they call and store under `enc._mergeable_ranks`. Feel free to copy paste the `recover_merges` function in `minbpe/gpt4.py`, which takes these ranks and returns the raw merges. If you wish to know how this function works, read [this](https://github.com/openai/tiktoken/issues/60) and [this](https://github.com/karpathy/minbpe/issues/11#issuecomment-1950805306). Basically, under some conditions it is enough to only store the parent nodes (and their rank) and get rid of the precise details of which children merged up to any parent.\n",
    "2. Second, the GPT-4 tokenizer for some reason permutes its raw bytes. It stores this permutation in the first 256 elements of the mergeable ranks, so you can recover this byte shuffle relatively simply as `byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}`. In both your encode and decode, you'll have to shuffle bytes around accordingly. If you're stuck, reference the minbpe/gpt4.py` file for hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f83aceaf-7c1e-4634-9557-04b7633f3214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# ---------- Copied from `minbpe/gpt4.py` under original repo ----------\n",
    "def bpe(mergeable_ranks, token, max_rank):\n",
    "    # helper function used in get_gpt4_merges() to reconstruct the merge forest\n",
    "    parts = [bytes([b]) for b in token]\n",
    "    while True:\n",
    "        min_idx = None\n",
    "        min_rank = None\n",
    "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
    "            if rank is not None and (min_rank is None or rank < min_rank):\n",
    "                min_idx = i\n",
    "                min_rank = rank\n",
    "        if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n",
    "            break\n",
    "        assert min_idx is not None\n",
    "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2:]\n",
    "    return parts\n",
    "\n",
    "\n",
    "def recover_merges(mergeable_ranks):\n",
    "    # the `merges` are already the byte sequences in their merged state.\n",
    "    # so we have to recover the original pairings. We can do this by doing\n",
    "    # a small BPE training run on all the tokens, in their order.\n",
    "    # also see https://github.com/openai/tiktoken/issues/60\n",
    "    # also see https://github.com/karpathy/minbpe/issues/11#issuecomment-1950805306\n",
    "    merges = {}\n",
    "    for token, rank in mergeable_ranks.items():\n",
    "        if len(token) == 1:\n",
    "            continue # skip raw bytes\n",
    "        pair = tuple(bpe(mergeable_ranks, token, max_rank=rank))\n",
    "        assert len(pair) == 2\n",
    "        # recover the integer ranks of the pair\n",
    "        ix0 = mergeable_ranks[pair[0]]\n",
    "        ix1 = mergeable_ranks[pair[1]]\n",
    "        merges[(ix0, ix1)] = rank\n",
    "\n",
    "    return merges\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "class GPT4Tokenizer:\n",
    "    INITIAL_VOCAB_SIZE = 256\n",
    "    \n",
    "    def __init__(self):\n",
    "        # get the official tokenizer and its merges\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        mergeable_ranks = enc._mergeable_ranks\n",
    "        self.merges = recover_merges(mergeable_ranks)\n",
    "        self.vocab = {i:i.to_bytes() for i in range(self.INITIAL_VOCAB_SIZE)}\n",
    "        for pair, tok in self.merges.items():\n",
    "            self.vocab[tok] = self.vocab[pair[0]] + self.vocab[pair[1]]\n",
    "        self.byte_shuffle = {i: enc._mergeable_ranks[bytes([i])] for i in range(256)}\n",
    "        self.inverse_byte_shuffle = {v:k for k,v in self.byte_shuffle.items()}\n",
    "\n",
    "    def train(self, text):\n",
    "        raise NotImplementedError(\"This is a pretrained tokenizer, not meant to be trained\")\n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\" text -> bytes using self.vocab and self.merges \"\"\"\n",
    "        ids = [self.byte_shuffle[b] for b in text.encode(\"utf-8\")]\n",
    "        # perform all merges\n",
    "        while len(ids) > 1:\n",
    "            pair_frequency = get_stats(ids)\n",
    "            # since a new token may be merged with a subsequent token, we must process lowest index tokens first\n",
    "            pair_to_merge = min(pair_frequency, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair_to_merge not in self.merges:\n",
    "                break\n",
    "            ids = merge(ids, pair_to_merge, self.merges[pair_to_merge])\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\" bytes -> text using self.vocab and self.merges \"\"\"\n",
    "        text_bytes = b\"\".join([self.vocab[tok] for tok in ids])\n",
    "        text_bytes = bytes([self.inverse_byte_shuffle[b] for b in text_bytes])\n",
    "        return text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c4e9a36-c86e-4e8d-9517-f0b89c1fa80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "gpt4_ids = enc.encode(\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\")\n",
    "gpt4_text = enc.decode(gpt4_ids) # get the same text back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf141ca0-8d98-401e-ada9-c323f8b44e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = GPT4Tokenizer()\n",
    "ids = tok.encode(\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\")\n",
    "text = tok.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32fa7d1e-5df1-419a-a211-a34fa414cff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(gpt4_ids == ids)\n",
    "print(gpt4_text == text)\n",
    "print(text == \"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a010f1-2ca0-4452-b857-92820c7b2779",
   "metadata": {},
   "source": [
    "### Step 4 (Will not do)\n",
    "\n",
    "(Optional, irritating, not obviously useful) Add the ability to handle special tokens. You'll then be able to match the output of tiktoken even when special tokens are present, e.g.:\n",
    "\n",
    "```\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "ids = enc.encode(\"<|endoftext|>hello world\", allowed_special=\"all\")\n",
    "```\n",
    "\n",
    "Without `allowed_special` tiktoken will error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8545058-cc79-4de8-b238-7151029ea41a",
   "metadata": {},
   "source": [
    "### Step 5 (Will not do)\n",
    "\n",
    "\n",
    "If you've made it this far, you're now a pro at LLM Tokenization! Sadly, you're not exactly done yet because a lot of LLMs outside of OpenAI (e.g. Llama, Mistral) use [sentencepiece](https://github.com/google/sentencepiece) instead. Primary difference being that sentencepiece runs BPE directly on Unicode code points instead of on UTF-8 encoded bytes. Feel free to explore sentencepiece on your own (good luck, it's not too pretty), and stretch goal if you really experience and suffer from the burden of time, re-write your BPE to be on Unicode code points and match the Llama 2 tokenizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
